{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iam-Iman/sync_interns_task_1/blob/main/task_1_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Chatbot"
      ],
      "metadata": {
        "id": "RGlwcCbNyvEE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE6w_F-qqB4q"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd2W8ssRf_0D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_tCdRPnqG9G"
      },
      "source": [
        "## Importing and reading the document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOao5BfLf1AJ",
        "outputId": "a14cb71a-d60f-4ffc-ac81-486776968c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "f=open('mood_crew.txt', 'r', errors='ignore')\n",
        "document = f.read()\n",
        "# convert to lower case\n",
        "document = document.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# convert document to list of sentences\n",
        "sent_tokens = nltk.sent_tokenize(document)\n",
        "# convert document to list of words\n",
        "word_tokens =  nltk.word_tokenize(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whP29NCwhrQq"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD24aEF4h47D"
      },
      "outputs": [],
      "source": [
        "lem = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lem.lemmatize(token) for token in tokens]\n",
        "remove_punctu_dict = dict((ord(punctu), None) for punctu in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punctu_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skPPwV0Hi5JO"
      },
      "source": [
        "## Building a greeting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mphRtrfUi_d6"
      },
      "outputs": [],
      "source": [
        "greet_inputs = ('hi', 'hey', 'hello')\n",
        "greet_responses = ['hola', 'yebo', 'hi']\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxvWOLiejuU_"
      },
      "source": [
        "## Response Generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS5MBW85jtt8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iGBv3B-kGs3"
      },
      "outputs": [],
      "source": [
        "def response(user_responses):\n",
        "  chat_response = ''\n",
        "  TfidfVect = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
        "  tfidf = TfidfVect.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf == 0):\n",
        "    chat_response = chat_response + 'Invalid text.Try again'\n",
        "    return chat_response\n",
        "  else:\n",
        "    chat_response = chat_response + sent_tokens[idx]\n",
        "    return chat_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFyQcuJFim5s"
      },
      "source": [
        "## Conversation Protocol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmOXMHhqinZR",
        "outputId": "e843e4bb-f475-481b-fb15-44fa85190f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I am Echo. Let us have a chat. If you want to stop the chat, type bye.\n",
            "Me: hello\n",
            "Chatbot: hi\n",
            "Me: happy\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy is able to put things in perspective\n",
            "and see the positive in situations.\n",
            "Me: the mood happy\n",
            "Chatbot: happy is able to put things in perspective\n",
            "and see the positive in situations.\n",
            "Me: when do we feel angry\n",
            "Chatbot: times we might feel angry:\n",
            "when we don’t get to play\n",
            "because we have to do our\n",
            "school work\n",
            "when people misunderstand\n",
            "what we are saying\n",
            "when something doesn’t go\n",
            "well for us\n",
            "\n",
            "when we feel angry,\n",
            "our body might feel like:\n",
            "our face is hot and red\n",
            "our muscles are tight\n",
            "and tense\n",
            "our heartbeat is faster\n",
            "than normal\n",
            "\n",
            "angry\n",
            "feeling angry is when we feel mad\n",
            "or upset about something.\n",
            "Me: when do we feel calm\n",
            "Chatbot: times we might feel calm:\n",
            "when we are falling asleep\n",
            "at night\n",
            "when we are doing something\n",
            "relaxing like coloring\n",
            "when we are with the people\n",
            "we love like our friends and\n",
            "family\n",
            "when we feel calm,\n",
            "our body might feel like:\n",
            "our muscles are relaxed\n",
            "we are able to listen\n",
            "and focus\n",
            "our heartbeat feels steady\n",
            "Me: tell me about excited\n",
            "Chatbot: favorite color: gray\n",
            "favorite food: hot dogs\n",
            "what i’m like: awkward\n",
            "around most people,\n",
            "but trying to change\n",
            "things i like to do:\n",
            "bird watching,\n",
            "hide-and-seek\n",
            "\n",
            "excited, is known for his upbeat\n",
            "personality and readiness to dive into\n",
            "any new opportunity.\n",
            "Me: bye\n",
            "Chatbot: Goodbye. Until next time.\n"
          ]
        }
      ],
      "source": [
        "flag = True\n",
        "print('Chatbot: I am Echo. Let us have a chat. If you want to stop the chat, type bye.')\n",
        "while(flag == True):\n",
        "  user_response = input('Me: ')\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response != 'bye'):\n",
        "    if(user_response == 'thank you' or user_response == 'thanks'):\n",
        "      flag = False\n",
        "      print('Chatbot: You are welcome.')\n",
        "    else:\n",
        "      if(greet(user_response) != None):\n",
        "        print('Chatbot: ' + greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        words = list(set(word_tokens))\n",
        "        print('Chatbot: ', end = '')\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print('Chatbot: Goodbye. Until next time.')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLOXN8DQuZ3sQQuXe5KZmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}